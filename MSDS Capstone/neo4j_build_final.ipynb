{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "169c3dee-0ca4-4ee7-891a-e392c83a7193",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pygraphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygraphviz\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mholoviews\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhv\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mholoviews\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dim, opts\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pygraphviz'"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Visualization packages\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pygraphviz\n",
    "import holoviews as hv\n",
    "from holoviews import dim, opts\n",
    "hv.extension('bokeh', 'matplotlib')\n",
    "import hvplot.networkx as hvnx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "import geopandas\n",
    "import contextily as ctx\n",
    "from shapely.geometry import Point\n",
    "import geoviews as gv\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c777f-053e-4827-89f2-5151f0a27c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSA dataframe represents the nodes\n",
    "# attributes: msa, main_city, population, gdp, tti, lat, lon\n",
    "msa_pickle = '../data/pickled/msa_df.pickle'\n",
    "with open(msa_pickle, 'rb') as file:\n",
    "    msa_df = pickle.load(file)\n",
    "msa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47decc4f-196f-49ee-bf0e-36e34122e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Route CONNECTIVITY relationship\n",
    "# # attributes: drive_distance, drive_duration, flight_frequency, flight_distance, flight_duration_ramp, flight_duration_air, \n",
    "# #             flight_full_duration, passenger_volume, capacity, carriers\n",
    "         \n",
    "# # Driving distance based on averages from Google Maps\n",
    "# dist_pickle = '../data/pickled/distance_df.pickle'\n",
    "# with open(dist_pickle, 'rb') as file:\n",
    "#     dist_df = pickle.load(file)\n",
    "\n",
    "# # Create dataframe to look at city pairs\n",
    "# city_pair_drive = dist_df.copy()\n",
    "# city_pair_drive['CityPair'] = city_pair_drive.apply(lambda x: tuple(sorted([x['Origin'], x['Destination']])), axis=1)\n",
    "# city_pair_drive = city_pair_drive.drop(columns=['Origin','Destination'])\n",
    "# # Groupby\n",
    "# city_pair_drive = city_pair_drive.groupby('CityPair').mean().reset_index()\n",
    "# city_pair_drive['City1'] = city_pair_drive['CityPair'].apply(lambda x: x[0])\n",
    "# city_pair_drive['City2'] = city_pair_drive['CityPair'].apply(lambda x: x[1])\n",
    "# city_pair_drive = city_pair_drive[['City1','City2','Distance_miles','Duration_minutes']].rename(columns={'Distance_miles':'drive_distance', 'Duration_minutes':'drive_duration'})\n",
    "\n",
    "# flight_pickle = '../data/pickled/flights_df.pickle'\n",
    "# with open(flight_pickle, 'rb') as file:\n",
    "#     flights_df = pickle.load(file)\n",
    "\n",
    "# flights_df['City1'] = flights_df['CityPair'].apply(lambda x: x[0])\n",
    "# flights_df['City2'] = flights_df['CityPair'].apply(lambda x: x[1])\n",
    "\n",
    "# city_pair_flight = flights_df.groupby(['City1','City2']).agg(\n",
    "#     num_passengers=('PASSENGERS','sum'),\n",
    "#     total_seats=('SEATS','sum'),\n",
    "#     flight_frequency=('DEPARTURES_PERFORMED', 'sum'),\n",
    "#     scheduled=('DEPARTURES_SCHEDULED','sum'),\n",
    "#     flight_distance=('DISTANCE','mean'),\n",
    "#     num_carriers=('UNIQUE_CARRIER','nunique'),\n",
    "#     flight_duration_ramp=('RAMP_TO_RAMP','sum'),\n",
    "#     flight_duration_air=('AIR_TIME','sum')\n",
    "# ).reset_index()\n",
    "\n",
    "# city_pair_flight['flight_duration_ramp'] = city_pair_flight['flight_duration_ramp']/city_pair_flight['flight_frequency']\n",
    "# city_pair_flight['flight_duration_air'] = city_pair_flight['flight_duration_air']/city_pair_flight['flight_frequency']\n",
    "# city_pair_flight['flight_duration_total'] = city_pair_flight['flight_duration_ramp'] + 200\n",
    "\n",
    "# city_pair = pd.merge(city_pair_drive,city_pair_flight, on=['City1', 'City2'], how='outer')\n",
    "# city_pair = city_pair[['City1', 'City2','drive_distance','drive_duration','flight_distance','flight_duration_ramp','flight_duration_air',\n",
    "#               'flight_duration_total','num_passengers','total_seats','flight_frequency','scheduled','num_carriers']]\n",
    "\n",
    "# # Get the \"as the crow flies\" distance for flight distance if we don't have flight data\n",
    "# def get_acf_dist(row,msa_df):\n",
    "#     if pd.isnull(row['flight_distance']):\n",
    "#         c1 = msa_df.loc[msa_df['MainCity']==row['City1'], ['lat','lng']]\n",
    "#         c1 = (c1['lat'].values[0],c1['lng'].values[0])\n",
    "#         c2 = msa_df.loc[msa_df['MainCity']==row['City2'], ['lat','lng']]\n",
    "#         c2 = (c2['lat'].values[0],c2['lng'].values[0])\n",
    "#         dist = geodesic(c1,c2).miles\n",
    "#         return dist\n",
    "#     return row['flight_distance']\n",
    "\n",
    "# # Apply the function to calculate the distance for null values\n",
    "# city_pair['flight_distance'] = city_pair.apply(lambda row: get_acf_dist(row,msa_df), axis=1)\n",
    "\n",
    "# # Drop Hawaii as it is not a viable option for HSR and fill all other nulls with 0\n",
    "# city_pair = city_pair[city_pair['drive_distance'].notnull()]\n",
    "# city_pair.fillna(0,inplace=True)\n",
    "\n",
    "# # Save the DataFrame as a pickle file\n",
    "# city_pair_pickle = '../data/pickled/city_pair_df.pickle'\n",
    "# with open(city_pair_pickle, 'wb') as file:\n",
    "#     pickle.dump(city_pair, file)\n",
    "\n",
    "city_pair_pickle = '../data/pickled/city_pair_df.pickle'\n",
    "with open(city_pair_pickle, 'rb') as file:\n",
    "    city_pair = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8567109a-6100-4730-8130-da6a4252232a",
   "metadata": {},
   "source": [
    "### Normalize variables\n",
    "#### Use min-max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ffc979-5bc2-495d-8e67-4ef55f4aeffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_df['PopNorm'] = (msa_df['Population'] - msa_df['Population'].min()) / (msa_df['Population'].max() - msa_df['Population'].min())\n",
    "msa_df['GDPNorm'] = (msa_df['GDP'] - msa_df['GDP'].min()) / (msa_df['GDP'].max() - msa_df['GDP'].min())\n",
    "msa_df['TTINorm'] = (msa_df['TravelTimeIndexValue'] - msa_df['TravelTimeIndexValue'].min()) / (msa_df['TravelTimeIndexValue'].max() - msa_df['TravelTimeIndexValue'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd1cdc-e942-409d-99b1-d9ee220a5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cn in city_pair.columns:\n",
    "    if 'City' in cn:\n",
    "        continue\n",
    "    norm = cn+'_norm'\n",
    "    city_pair[norm] = (city_pair[cn] - city_pair[cn].min()) / (city_pair[cn].max() - city_pair[cn].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8877a5-cbf6-4027-a708-1dfdc18a5c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jConnection:\n",
    "    \n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = pwd\n",
    "        self.__driver = None\n",
    "        try:\n",
    "            self.__driver = GraphDatabase.driver(self.__uri, auth=(self.__user, self.__pwd))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create the driver:\", e)\n",
    "        \n",
    "    def close(self):\n",
    "        if self.__driver is not None:\n",
    "            self.__driver.close()\n",
    "        \n",
    "    def query(self, query, data=None, db=None):\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try: \n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session() \n",
    "            response = list(session.run(query,data))\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally: \n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c7bfbb-66a8-41b6-a6ff-213dabde6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = \"bolt://localhost:7687\"\n",
    "USERNAME = \"neo4j\"\n",
    "PASSWORD = \"HSRProject\"\n",
    "conn = Neo4jConnection(URI, USERNAME, PASSWORD)\n",
    "\n",
    "conn.query(\"CREATE OR REPLACE DATABASE hsrdb\",{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a3ed1-0805-4ea5-b0c3-b47b3076e9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add MSAs as nodes\n",
    "msa_dict = msa_df.to_dict(orient='records')\n",
    "\n",
    "query = '''\n",
    "    UNWIND $rows AS row\n",
    "    MERGE (m:MSA {name: row.MainCity})\n",
    "    SET m.msa = row.MetroArea, \n",
    "        m.population = row.Population, \n",
    "        m.pop_norm = row.PopNorm,\n",
    "        m.gdp = row.GDP,\n",
    "        m.gdp_norm = row.GDPNorm,\n",
    "        m.tti = row.TravelTimeIndexValue,\n",
    "        m.tti_norm = row.TTINorm,\n",
    "        m.lon = row.lng, \n",
    "        m.lat = row.lat\n",
    "    RETURN count(*) as total\n",
    "    '''\n",
    "conn.query(query, data={\"rows\": msa_dict}, db='hsrdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527afa87-b095-4209-a83e-1d6b77410763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add city_pairs as edges\n",
    "cp_dict = city_pair.to_dict(orient='records')\n",
    "\n",
    "query = '''\n",
    "    UNWIND $rows as row\n",
    "    MATCH (m1:MSA {name: row.City1})\n",
    "    MATCH (m2:MSA {name: row.City2})\n",
    "    MERGE (m1)<-[:CONNECTIVITY {\n",
    "        drive_distance: row.drive_distance, \n",
    "        drive_duration: row.drive_duration,\n",
    "        flight_distance: row.flight_distance,\n",
    "        flight_duration_ramp: row.flight_duration_ramp,\n",
    "        flight_duration_air: row.flight_duration_air,\n",
    "        flight_duration_total: row.flight_duration_total,\n",
    "        num_passengers: row.num_passengers,\n",
    "        total_seats: row.total_seats,\n",
    "        flight_frequency: row.flight_frequency,\n",
    "        scheduled: row.scheduled,\n",
    "        num_carriers: row.num_carriers,\n",
    "        drive_distance_norm: row.drive_distance_norm, \n",
    "        drive_duration_norm: row.drive_duration_norm,\n",
    "        flight_distance_norm: row.flight_distance_norm,\n",
    "        flight_duration_ramp_norm: row.flight_duration_ramp_norm,\n",
    "        flight_duration_air_norm: row.flight_duration_air_norm,\n",
    "        flight_duration_total_norm: row.flight_duration_total_norm,\n",
    "        num_passengers_norm: row.num_passengers_norm,\n",
    "        total_seats_norm: row.total_seats_norm,\n",
    "        flight_frequency_norm: row.flight_frequency_norm,\n",
    "        scheduled_norm: row.scheduled_norm,\n",
    "        num_carriers_norm: row.num_carriers_norm}]->(m2)\n",
    "   '''\n",
    "conn.query(query, data={\"rows\": cp_dict}, db='hsrdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fabcaa5-1b1a-446d-8242-7b3d90a6da3b",
   "metadata": {},
   "source": [
    "## Initial Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab553565-3594-4dcf-9e37-63c3a31fba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_query = '''\n",
    "    MATCH (n:MSA)\n",
    "    RETURN n.name AS name, n.lat AS lat, n.lon AS lon\n",
    "'''\n",
    "nodes_results = conn.query(nodes_query, db='hsrdb')\n",
    "nodes_df = pd.DataFrame([dict(record) for record in nodes_results])\n",
    "\n",
    "# It is super big to draw all connections, since is is a fully connected/complete graph (all nodes in the network are connected to all other nodes)\n",
    "# Filter to view only relationships that include flight data\n",
    "rels_query = \"\"\"\n",
    "    MATCH (n)-[r]->(m)\n",
    "    WHERE r.num_passengers > 0\n",
    "    RETURN n, r, m\n",
    "\"\"\"\n",
    "rels_results = conn.query(rels_query, db='hsrdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf829c1-3a1d-4617-9b16-e35a42f88abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes from nodes_results\n",
    "for record in nodes_results:\n",
    "    G.add_node(record['name'], lat=record['lat'], lon=record['lon'])\n",
    "\n",
    "# Add edges from relationships_results\n",
    "for record in rels_results:\n",
    "    node1 = record['n']\n",
    "    node2 = record['m']\n",
    "    rel = record['r']\n",
    "    G.add_edge(node1['name'], node2['name'], relationship=rel.type)\n",
    "\n",
    "# Calculate node degrees and determine sizes\n",
    "degrees = dict(G.degree())\n",
    "max_degree = max(degrees.values()) if degrees else 1  # Avoid division by zero\n",
    "nodes_df['degree'] = nodes_df['name'].map(degrees)\n",
    "nodes_df['size'] = nodes_df['degree'].apply(lambda x: 5 + (x / max_degree) * 15)  # Scale size between 5 and 20\n",
    "\n",
    "# Create a GeoDataFrame for nodes\n",
    "gdf = geopandas.GeoDataFrame(nodes_df, geometry=[Point(xy) for xy in zip(nodes_df['lon'], nodes_df['lat'])])\n",
    "\n",
    "# Create GeoViews points\n",
    "points = gv.Points(gdf, vdims=['name','size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac9a1ef-a986-4a74-97f0-678075c41471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the graph using geocoords\n",
    "# Plot using Holoviews\n",
    "plot = points.opts(\n",
    "    opts.Points(\n",
    "        color='#008080', \n",
    "        size='size',  # Size of the points\n",
    "        tools=['hover'],  # Enable hover tool\n",
    "        width=800,\n",
    "        height=600,\n",
    "        title=\"MSA Graph\",\n",
    "        line_width=0.5\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot edges using Holoviews\n",
    "edges = []\n",
    "for u, v in G.edges():\n",
    "    line = [(G.nodes[u]['lon'], G.nodes[u]['lat']), (G.nodes[v]['lon'], G.nodes[v]['lat'])]\n",
    "    edges.append(line)\n",
    "\n",
    "edge_lines = gv.Path(edges)\n",
    "\n",
    "# Combine plots\n",
    "final_plot = edge_lines.opts(color='gray', line_width=0.5) * plot\n",
    "\n",
    "# Display the plot\n",
    "hv.output(final_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cc4f1b-9282-4068-910e-1df3643c2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the graph\n",
    "pos = graphviz_layout(G)\n",
    "node_sizes = [(5 + G.degree(node)) * 1 for node in G.nodes()]\n",
    "node_colors = ['#008080'] * len(G.nodes())\n",
    "plot_size=(800, 800)\n",
    "\n",
    "pos_df = pd.DataFrame.from_dict(pos, orient='index', columns=['x', 'y'])\n",
    "pos_df['index'] = pos_df.index\n",
    "\n",
    "plot = hvnx.draw(G, pos=pos, node_size=node_sizes, node_color=node_colors, edge_color=\"gray\", edge_line_width=1)\n",
    "plot.opts(width=plot_size[0], height=plot_size[1])\n",
    "\n",
    "#labels = hv.Labels(pos_df, ['x', 'y'], 'index')\n",
    "#plot = plot * labels.opts(xoffset=0, yoffset=-5)\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc8b9db-2651-4bf7-ae04-39f8f4b0d0f6",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e22fe1-e3be-4d63-9c91-f4acafdc1011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at travel demand\n",
    "td_query = '''\n",
    "    MATCH (m1:MSA)-[r:CONNECTIVITY]->(m2:MSA)\n",
    "    RETURN m1.name AS City1, m2.name AS City2,\n",
    "           r.num_passengers AS Passengers, r.total_seats AS Seats,\n",
    "           r.flight_frequency AS Frequency, r.num_carriers AS Airlines\n",
    "    ORDER BY Passengers DESC\n",
    "    LIMIT 10\n",
    "'''\n",
    "\n",
    "td_result = conn.query(td_query, db='hsrdb')\n",
    "td_df = pd.DataFrame([dict(record) for record in td_result])\n",
    "td_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144853b5-38e1-42dc-8cf8-4a29fda90f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximize passenger volume and minimize distance\n",
    "eff_query = '''\n",
    "    MATCH (m1:MSA)-[r:CONNECTIVITY]->(m2:MSA)\n",
    "    WHERE r.drive_distance_norm > 0\n",
    "    RETURN m1.name AS City1,\n",
    "           m2.name AS City2,\n",
    "           r.num_passengers AS PassengerVolume,\n",
    "           r.drive_distance AS DriveDistance,\n",
    "           (r.num_passengers_norm / r.drive_distance_norm) AS EfficiencyScore\n",
    "    ORDER BY EfficiencyScore DESC\n",
    "'''\n",
    "\n",
    "eff_result = conn.query(eff_query, db='hsrdb')\n",
    "eff_df = pd.DataFrame([dict(record) for record in eff_result])\n",
    "\n",
    "# write dataframe to csv\n",
    "eff_df.to_csv('efficiencyScore.csv',index=False)\n",
    "\n",
    "eff_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e8d92-b8c6-4065-bd25-18024e9b80ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_pair_weights_df = pd.merge(city_pair, eff_df, on=['City1','City2'], how='left').drop(columns=['PassengerVolume','DriveDistance']).rename(columns={'EfficiencyScore':'efficiency_score'})\n",
    "city_pair_weights_df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2fcf2-d3ea-4832-b747-341d120e9e84",
   "metadata": {},
   "source": [
    "## Degree Centrality\n",
    "#### Centrality can help identify key hubs that could maximize network connectivity if connected by HSR.\n",
    "\n",
    "#### Measures the number of connections (edges) a node has. In a weighted context, it can be interpreted as the sum of the weights of all edges connected to the node.  \n",
    "\n",
    "#### When weights are used, such as num_passengers, the score reflects the total volume of these weights associated with a city. It effectively captures the overall travel demand involving that city.\n",
    "\n",
    "#### Create a custom weight to use by combining features:\n",
    "*demand_weight = num_passengers * 0.8 + flight_frequency * 0.2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d7a340-e639-4637-a3eb-8b1abfa506ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a demand weight using num_passengers and flight freq\n",
    "create_dw_query = '''\n",
    "    MATCH (m1:MSA)-[r:CONNECTIVITY]->(m2:MSA)\n",
    "    SET r.demand_weight = (r.num_passengers_norm * 0.7) + (r.flight_frequency_norm * 0.3)\n",
    "    RETURN m1.name AS City1,\n",
    "           m2.name AS City2,\n",
    "           r.demand_weight AS DemandWeight\n",
    "    ORDER BY DemandWeight DESC\n",
    "'''\n",
    "\n",
    "dw_result = conn.query(create_dw_query, db='hsrdb')\n",
    "dw_df = pd.DataFrame([dict(record) for record in dw_result])\n",
    "dw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7efe4c5-3b6b-4a85-b961-b08657868e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at MSAs with high degree centrailty score\n",
    "demand_proj_query = '''\n",
    "    CALL gds.graph.project(\n",
    "        'demandGraph',\n",
    "        'MSA',\n",
    "        {\n",
    "            CONNECTIVITY: {\n",
    "                orientation: 'UNDIRECTED',\n",
    "                properties: ['demand_weight']\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    YIELD\n",
    "    graphName AS graph,\n",
    "    relationshipProjection AS knowsProjection,\n",
    "    nodeCount AS nodes,\n",
    "    relationshipCount AS rels\n",
    "'''\n",
    "\n",
    "conn.query(demand_proj_query, db='hsrdb')\n",
    "\n",
    "# Calculate degree centrality using custom weight\n",
    "demand_deg_query = '''\n",
    "    CALL gds.degree.stream('demandGraph', { relationshipWeightProperty: 'demand_weight' })\n",
    "    YIELD nodeId, score\n",
    "    RETURN gds.util.asNode(nodeId).name AS MSA, score AS DemandScore\n",
    "    ORDER BY score DESC\n",
    "'''\n",
    "\n",
    "demand_deg_result = conn.query(demand_deg_query, db='hsrdb')\n",
    "demand_deg_df = pd.DataFrame([dict(record) for record in demand_deg_result])\n",
    "demand_deg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08cef7-56a2-4b1f-bf4d-9ce1df8a12fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_deg_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9296855-8f86-48a9-a367-8694639ccacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add demand calculations to data frames\n",
    "city_pair_weights_df = pd.merge(city_pair_weights_df, dw_df, on=['City1', 'City2'], how='left').rename(columns={'DemandWeight':'demand_weight'})\n",
    "msa_weights_df = pd.merge(msa_df,demand_deg_df, left_on='MainCity', right_on='MSA', how='left').rename(columns={'DemandScore':'demand_score'})\n",
    "msa_weights_df.drop(columns=['MSA'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05469fb5-e99b-4cc1-b5e1-f88e992f4b29",
   "metadata": {},
   "source": [
    "## Closeness Centrality\n",
    "#### Closeness centrality measures how quickly a node can reach all other nodes in the network. It is defined as the inverse of the sum of the shortest path distances from the node to all other nodes in the graph. Nodes with a high closeness score have the shortest distances to all other nodes.\n",
    "\n",
    "#### Can help identify cities that are centrally located in terms of travel time or distance, making them potentially good candidates for hubs.\n",
    "\n",
    "#### ** Could be really useful if we first cluster cities and then determine closeness centrality.\n",
    "#### First calculate overall score, then also calculate clustered score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a0cd8-8fc2-47eb-8191-8737699f7838",
   "metadata": {},
   "source": [
    "#### Create custom closeness measure using weights\n",
    "Since the default closeness centrality doesn’t consider weights, you can implement a custom measure to approximate proximity by considering the weighted paths between cities.\n",
    "\n",
    "Inverse Distance: Use the inverse of the travel distance as a measure of proximity, giving higher scores to cities that are closer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2f13f-d024-4cf1-8ecf-cffea0c5a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_distw_query = '''\n",
    "    MATCH (m1:MSA)-[r:CONNECTIVITY]->(m2:MSA)\n",
    "    WITH m1.name AS City1, m2.name AS City2, \n",
    "         CASE \n",
    "           WHEN r.drive_distance > 0 THEN 1.0 / r.drive_distance\n",
    "           ELSE 0\n",
    "         END AS proximity_score\n",
    "    \n",
    "    // Aggregate the proximity scores for City1\n",
    "    WITH City1, proximity_score\n",
    "    RETURN City1 AS City, sum(proximity_score) AS TotalProximityScore\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    // Aggregate the proximity scores for City2\n",
    "    MATCH (m1:MSA)-[r:CONNECTIVITY]->(m2:MSA)\n",
    "    WITH m1.name AS City1, m2.name AS City2, \n",
    "         CASE \n",
    "           WHEN r.drive_distance > 0 THEN 1.0 / r.drive_distance\n",
    "           ELSE 0\n",
    "         END AS proximity_score\n",
    "    \n",
    "    WITH City2, proximity_score\n",
    "    RETURN City2 AS City, sum(proximity_score) AS TotalProximityScore\n",
    "'''\n",
    "distw_result = conn.query(create_distw_query, db='hsrdb')\n",
    "distw_df = pd.DataFrame([dict(record) for record in distw_result])\n",
    "\n",
    "final_scores = distw_df.groupby('City').sum().reset_index()\n",
    "final_scores = final_scores.sort_values(by='TotalProximityScore', ascending=False)\n",
    "final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643bec17-4e21-498d-897b-c4b1b2cc11b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_weights_df = pd.merge(msa_weights_df,final_scores, left_on='MainCity', right_on='City', how='left').rename(columns={'TotalProximityScore':'proximity_score'})\n",
    "msa_weights_df.drop(columns=['City'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db7f42-1539-4dfa-b42e-2ad6ba50e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f2f715-efcf-4a69-bb5c-0921a821c376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "309414bf-1590-4009-9ade-3893ed8da2ea",
   "metadata": {},
   "source": [
    "## Community Detection/Clustering\n",
    "#### Find clusters of cities with strong connectivity to identify potential regions for HSR. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18378ec6-6040-4ee1-b48f-3db10a4ede06",
   "metadata": {},
   "source": [
    "### Approach 1: \n",
    "#### Simply cluster nodes using lat/lon (k-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33291e93-d8de-4cb2-934a-dd8646b6dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put lat/lon in single field\n",
    "coord_query = '''\n",
    "    MATCH (m:MSA)\n",
    "    SET m.coordinates = [m.lat, m.lon]\n",
    "'''\n",
    "conn.query(coord_query, db='hsrdb')\n",
    "\n",
    "geo_proj = '''                \n",
    "    MATCH (m1:MSA)-[r:CONNECTIVITY]->(m2:MSA)\n",
    "    WITH gds.graph.project(\n",
    "        'geoGraph',\n",
    "        m1,\n",
    "        m2,\n",
    "        {\n",
    "            sourceNodeProperties: m1 {coordinates: m1.coordinates},\n",
    "            targetNodeProperties: m2 {coordinates: m2.coordinates}\n",
    "        }\n",
    "            \n",
    "    ) AS g\n",
    "    RETURN g.graphName, g.nodeCount, g.relationshipCount\n",
    "'''\n",
    "conn.query(geo_proj, db='hsrdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4896d54-58b2-45c3-8ef1-5c110167a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_query = '''\n",
    "    CALL gds.kmeans.stream('geoGraph', {\n",
    "        nodeProperty: 'coordinates',\n",
    "        k: 6\n",
    "    })\n",
    "    YIELD nodeId, communityId\n",
    "    RETURN gds.util.asNode(nodeId).name AS name, gds.util.asNode(nodeId).lat AS lat, gds.util.asNode(nodeId).lon AS lon, communityId\n",
    "    ORDER BY communityId, name ASC\n",
    "'''\n",
    "kmeans_result = conn.query(kmeans_query, db='hsrdb')\n",
    "kmeans_df = pd.DataFrame([dict(record) for record in kmeans_result])\n",
    "kmeans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67f686-58a3-46c8-84c3-827f64ebf427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes from nodes_results\n",
    "for record in kmeans_result:\n",
    "    G.add_node(record['name'], pos=(record['lon'], record['lat']), cluster=record['communityId'])\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "gdf = geopandas.GeoDataFrame(kmeans_df, geometry=[Point(xy) for xy in zip(kmeans_df['lon'], kmeans_df['lat'])])\n",
    "\n",
    "# Convert the GeoDataFrame to Holoviews Elements\n",
    "points = gv.Points(gdf, vdims=['name', 'communityId'])\n",
    "\n",
    "palette = ['red', 'blue', 'green', 'orange', 'purple','pink']\n",
    "          \n",
    "# Visualize using Holoviews\n",
    "plot = points.opts(\n",
    "    opts.Points(\n",
    "        color='communityId',  # Use ClusterId for coloring\n",
    "        cmap=palette,  # Define the color map\n",
    "        size=8,  # Size of the points\n",
    "        tools=['hover'],  # Enable hover tool\n",
    "        width=800,\n",
    "        height=600,\n",
    "        title=\"City Clusters by ClusterId\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d7698-4dc8-4a26-af61-03077bb203a7",
   "metadata": {},
   "source": [
    "### Approach 2:\n",
    "#### Perform community detection using drive and flight distance (Leiden). To do this, we use Gaussian weighting for distance based on our optimal range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2615fcad-e687-4c86-9ed0-3290cbe94fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gaus_weights = '''\n",
    "    // Define parameters\n",
    "    WITH 300.0 AS optimalDistance, 25.0 AS sigma\n",
    "\n",
    "    // Calculate Gaussian weights for both drive and flight distances\n",
    "    MATCH (m1:MSA)-[r:CONNECTIVITY]->(m2:MSA)\n",
    "    WITH m1, m2, r, \n",
    "        exp(-((r.drive_distance - optimalDistance) * (r.drive_distance - optimalDistance)) / (2 * sigma * sigma)) AS driveWeight,\n",
    "        exp(-((r.flight_distance - optimalDistance) * (r.flight_distance - optimalDistance)) / (2 * sigma * sigma))AS flightWeight\n",
    "    SET r.dist_weight = (driveWeight*0.5) + (flightWeight*0.5)\n",
    "    RETURN m1.name AS City1, m2.name AS City2, r.drive_distance AS DriveDistance, r.flight_distance AS FlightDistance, r.dist_weight AS Weight\n",
    "    ORDER BY Weight desc\n",
    "'''\n",
    "\n",
    "gaus_dist_result = conn.query(create_gaus_weights, db='hsrdb')\n",
    "gaus_dist_df = pd.DataFrame([dict(record) for record in gaus_dist_result])\n",
    "# write dataframe to csv\n",
    "gaus_dist_df.to_csv('gaussianWeights.csv',index=False)\n",
    "gaus_dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4866cf-1f02-4af1-a5d9-e4158ad85108",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_pair_weights_df = pd.merge(city_pair_weights_df, gaus_dist_df, on=['City1', 'City2'], how='left').rename(columns={'Weight':'gaus_dist_weight'})\n",
    "city_pair_weights_df.drop(columns=['DriveDistance','FlightDistance'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad1e20-ec84-4e34-b193-5cc87dc48296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the connected component\n",
    "conn_comp = '''\n",
    "    MATCH (m1:MSA)-[r:CONNECTIVITY]->(m2:MSA)\n",
    "    WITH gds.graph.project(\n",
    "        'connectedMSAGraph',\n",
    "        m1,\n",
    "        m2,\n",
    "        { relationshipProperties: r { .dist_weight }},\n",
    "        { undirectedRelationshipTypes: ['*']}\n",
    "    ) AS g\n",
    "    RETURN\n",
    "        g.graphName AS graph, g.nodeCount AS nodes, g.relationshipCount AS rels\n",
    "'''\n",
    "conn.query(conn_comp, db='hsrdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e267cdb-c104-4a4b-a947-8230ffea7cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leiden community detection algorithm\n",
    "leiden_query = '''\n",
    "    CALL gds.leiden.stream('connectedMSAGraph',{ relationshipWeightProperty: 'dist_weight' })\n",
    "    YIELD nodeId, communityId\n",
    "    RETURN gds.util.asNode(nodeId).name AS name, gds.util.asNode(nodeId).lat AS lat, gds.util.asNode(nodeId).lon AS lon, communityId\n",
    "    ORDER BY communityId, name\n",
    "'''\n",
    "\n",
    "leiden_result = conn.query(leiden_query, db='hsrdb')\n",
    "leiden_df = pd.DataFrame([dict(record) for record in leiden_result])\n",
    "leiden_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8406d5b-e90d-4fd7-8a16-e2197e655b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_mapping = {old_id: new_id for new_id, old_id in enumerate(sorted(leiden_df['communityId'].unique()))}\n",
    "# Apply the mapping to the DataFrame\n",
    "leiden_df['mappedCommunityId'] = leiden_df['communityId'].map(community_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93db00-c0cc-49c3-8928-070d20da5b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes from nodes_results\n",
    "for record in leiden_result:\n",
    "    G.add_node(record['name'], pos=(record['lon'], record['lat']), cluster=record['communityId'])\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "gdf = geopandas.GeoDataFrame(leiden_df, geometry=[Point(xy) for xy in zip(leiden_df['lon'], leiden_df['lat'])])\n",
    "\n",
    "# Convert the GeoDataFrame to Holoviews Elements\n",
    "points = gv.Points(gdf, vdims=['name', 'mappedCommunityId'])\n",
    "\n",
    "palette = ['red', 'blue', 'green', 'orange', 'purple','pink']\n",
    "\n",
    "# Visualize using Holoviews\n",
    "plot = points.opts(\n",
    "    opts.Points(\n",
    "        color='mappedCommunityId',  # Use ClusterId for coloring\n",
    "        cmap=palette,  # Define the color map\n",
    "        size=8,  # Size of the points\n",
    "        tools=['hover'],  # Enable hover tool\n",
    "        width=800,\n",
    "        height=600,\n",
    "        title=\"City Clusters by Leiden Algorithm\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda01224-136f-4602-bd2b-347c56e7abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "leiden_df.drop(columns=['lat','lon'], inplace=True)\n",
    "msa_weights_df = pd.merge(msa_weights_df,leiden_df, left_on='MainCity', right_on='name', how='left').rename(columns={'mappedCommunityId':'cluster_id'})\n",
    "msa_weights_df.drop(columns=['name','communityId'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6894eb-f662-40e5-8731-32a722f74004",
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d5d565-b817-468c-b0ca-f67beaa60611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b58d1-2ff6-4fc7-bdd1-b305fd4ff6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrames as a pickle file\n",
    "city_pair_weights_pickle = '../data/pickled/city_pair_weights_df.pickle'\n",
    "with open(city_pair_weights_pickle, 'wb') as file:\n",
    "    pickle.dump(city_pair_weights_df, file)\n",
    "\n",
    "msa_weights_pickle = '../data/pickled/msa_weights_df.pickle'\n",
    "with open(msa_weights_pickle, 'wb') as file:\n",
    "    pickle.dump(msa_weights_df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a071f2-5cfb-41d8-ae8f-d50c6a6ca9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
